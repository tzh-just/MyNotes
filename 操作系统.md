# 硬件结构
## 内存
内存的地址是从 0 开始编号的，然后⾃增排列，最后⼀个地址为内存总字节数 - 1，这种结构好似我们程序 ⾥的数组，所以内存的读写任何⼀个数据的速度都是⼀样的。
## CPU
CPU 位宽越⼤，可以计算的数值就越⼤
CPU 内部还有⼀些组件，常⻅的有寄存器、控制单元和逻辑运算单元等。
CPU 中的寄存器主要作⽤是存储计算时的数据，
因为内存离 CPU 太远了，⽽寄存器就在 CPU ⾥，还紧挨着控制单元和逻辑运算单元，计算速度会更快
### 寄存器
- 通⽤寄存器，⽤来存放需要进⾏运算的数据，⽐如需要进⾏加和运算的两个数据。
- 程序计数器，⽤来存储 CPU 要执⾏下⼀条指令「所在的内存地址」，注意不是存储了下⼀条要执⾏ 的指令，此时指令还在内存中，程序计数器只是存储了下⼀条指令的地址。
- 指令寄存器，⽤来存放程序计数器指向的指令，也就是指令本身，指令被执⾏完成之前，指令都存储 在这⾥。
### 总线
- 地址总线，⽤于指定 CPU 将要操作的内存地址；
- 数据总线，⽤于读写内存的数据；
- 控制总线，⽤于发送和接收信号，⽐如中断、设备复位等信号，CPU 收到信号后⾃然进⾏响应，这时也需要控制总线；

当 CPU 要读写内存数据：
- ⾸先要通过「地址总线」来指定内存的地址；
- 再通过「数据总线」来传输数据；

### 输入输出设备
输⼊设备向计算机输⼊数据，计算机经过计算后，把数据输出给输出设备。
如果输⼊设备是键盘，按下按键时是需要和 CPU 进⾏交互的，这时就需要⽤到控制总线了。

### 线路位宽与CPU位宽
低电压表示 0，⾼压电压则表示 1。
如果只有⼀条线路，就意味着每次只能传递 1 bit 的数据，即 0 或 1。这样⼀位⼀位传输的⽅式，称为串⾏。
线路的位宽最好⼀次就能访问到所有的内存地址。如果想要 CPU 操作 4G 的内存，那么就需要 32 条地址总线，因为 2 ^ 32 =4G 。

### 程序执⾏的基本过程
⼀个程序执⾏的时候，CPU 会根据程序计数器⾥的内存地址，从内存⾥⾯把需要执⾏的指令读取到指令寄存器⾥⾯执⾏，然后根据指令⻓度⾃增，开始顺序读取下⼀条指令。
1. CPU 读取「程序计数器」的值，这个值是指令的内存地址，这个部分称为 Fetch（取得指令）
	1. CPU 的「控制单元」操作「地址总线」指定需要访问的内存地址
	2. 通知内存设备准备数据，数据准备好后通过「数据总线」将指令数据传给 CPU
	3. CPU 收到内存传来的数据后，将这个指令数据存⼊到「指令寄存器」
2. CPU 分析「指令寄存器」中的指令，确定指令的类型和参数，这个部分称为 Decode（指令译码）
	1. 如果是计算类型的指令，就把指令交给「逻辑运算单元」运算
	2. 如果是存储类型的指令，则交由「控制单元」执⾏
	3. 
3. CPU Execution（执⾏指令）后，「程序计数器」的值⾃增，表示指向下⼀条指令。
	1. ⾃增的⼤⼩由CPU 的位宽决定
	2. CPU 将计算结果存回寄存器或者将寄存器的值存⼊内存，这个部分称为 Store（数据回写）
CPU 从程序计数器读取指令、到执⾏、再到下⼀条指令，这个过程会不断循环，直到程序执⾏结束，这个不断循环的过程被称为 CPU 的指令周期。

### a = 1 + 2 执⾏具体过程
编译器通过分析代码，发现 1 和 2 是数据，于是程序运⾏时，内存会有个专⻔的区域来存放这些数据，这个区域就是「数据段」。注意，数据和指令是分开区域存放的，存放指令区域的地⽅称为「正⽂段」。
例如编译完成后，具体执⾏程序的时候，程序计数器会被设置为 0x200 地址，然后依次执⾏这 4 条指令。
- 0x200 的内容是 load 指令将 0x100 地址中的数据 1 装⼊到寄存器 R0
- 0x204 的内容是 load 指令将 0x104 地址中的数据 2 装⼊到寄存器 R1
- 0x208 的内容是 add 指令将寄存器 R0 和 R1 的数据相加，并把结果存放到寄存器 R2
- 0x20c 的内容是 store 指令将寄存器 R2 中的数据存回数据段中的 0x108 地址中，这个地址也就是变量 a 内存中的地址

事实上指令的内容是⼀串⼆进制数字的机器码，每条指令都有对应的机器码，CPU 通过解析机器码来知道指令的内容。
十六进制add指令0x00011020
- add 对应的 MIPS 指令⾥操作码是 000000 ，以及最末尾的功能码是 100000
- rs 代表第⼀个寄存器 R0 的编号，即 00000
- rt 代表第⼆个寄存器 R1 的编号，即 00001
- rd 代表⽬标的临时寄存器 R2 的编号，即 00010
- 因为不是位移操作，所以位移量是 00000
| 指令 | 指令类型 | 操作码6位 | rs5位 | rt五位 | rd5位 | 位移量5位 | 功能码6位 |
| ---- | -------- | --------- | ----- | ------ | ----- | --------- | --------- |
| add  | R        | 000000    | 00000 | 00001  | 00010 | 00000     | 100000          |

指令存放在存储器中，由控制器通过程序计数器和指令寄存器取出指令后译码为不同的操作信号、地址和操作数，操作如果是算数、逻辑、数据传输、条件分支都是算数逻辑单元操作，即运算器。如果是简单的无条件地址跳转则由控制器自己完成。


指令类型
- 数据传输类型的指令，store/load 是寄存器与内存间数据传输的指令， mov 是将⼀个内存地址的数据移动到另⼀个内存地址的指令
- 运算类型的指令，如加减乘除、位运算、⽐较⼤⼩等等，最多只能处理两个寄存器中的数据
- 跳转类型的指令，通过修改程序计数器的值来达到跳转执⾏指令的过程，如 if-else 、 swtich-case、函数调⽤等
- 信号类型的指令，⽐如发⽣中断的指令 trap
- 闲置类型的指令，⽐如指令 nop ，执⾏后 CPU 会空转⼀个周期
### 总结
>64 位相⽐ 32 位 CPU 的优势在哪吗？64 位 CPU 的计算性能⼀定⽐ 32 位 CPU ⾼很多吗？

64 位 CPU 可以⼀次计算超过 32 位的数字，⽽ 32 位 CPU 如果要计算超过 32 位的数字，要分多步骤进⾏计算，效率就没那么⾼。只有运算⼤数字的时候，64 位 CPU 的优势才能体现出来，否则和 32 位 CPU 的计算性能相差不⼤。

>软件的 32 位和 64 位之间的区别？32 位的操作系统可以运⾏在 64 位的电脑上吗？64 位的操作系统可以运⾏在 32 位的电脑上吗？

64 位和 32 位软件，实际上代表指令是 64 位还是 32 位的。如果 32 位指令在 64 位机器上执⾏，需要⼀套兼容机制，就可以做到兼容运⾏了。如果 64 位指令在 32 位机器上执⾏，就⽐较困难了，因为 32 位的寄存器存不下 64 位的指令。64 位操作系统，指令也就是 64 位，因此不能装在 32 位机器上。

## 存储器金字塔
CPU 中的寄存器，处理速度是最快的，但是能存储的数据也是最少的。
CPU Cache 通常会分为 L1、L2、L3 三层，其中 L1 Cache 通常分成「数据缓存」和「指令缓存」，L1是距离 CPU 最近的，因此它⽐ L2、L3 的读写速度都快、存储空间都⼩。
| 存储器 | 分布                         | 速度                    | 大小        |
| ------ | ---------------------------- | ----------------------- | ----------- |
| 寄存器 | ⼏⼗到⼏百个                 | 半个 CPU 时钟周期       | 根据CPU位宽 |
| L1     | 每个核心各一个数据和指令缓存 | 2~4时钟周期             | 几十KB      |
| L2     | 每个核心一块                 | 10~20 个时钟周期        | ⼏百 KB     |
| L3     | 核心共享                     | 20~60 个时钟周期        | 几MB        |
| 内存   |                              | 200~300 个 时钟周期之间 |             |
| SSD       |                |       内存的10到1000分之一              |             |

![](assets/Pasted%20image%2020230718101725.png)

CPU 需要访问内存中某个数据时，会从寄存器沿着金字塔往下查询。程序执行时，会将内存中的数据加载到共享的 L3 Cache 中，再加载到每个核⼼独有的 L2 Cache，最后进⼊到最快的 L1 Cache，之后才会被 CPU 读取。

### Cache Line
CPU Cache 的数据是从内存中读取过来的，它是以⼀⼩块⼀⼩块读取数据的，⽽不是按照单个数组元素来读取数据的，在 CPU Cache 中的，这样⼀⼩块⼀⼩块的数据，称为 Cache Line（缓存块）。
以 64 字节的 L1 Cache Line 为例，对于 int array[100] 的数组，当载⼊ array[0] 时，由于这个数组元素的⼤⼩在内存只占 4 字节，不⾜ 64 字节，CPU 就会顺序加载数组元素到 array[15] ， 即0-15的数组元素都被缓存在 CPU Cache 中。当下次访问这些数组元素时，会直接从 CPU Cache 读取，大大地提高了速度访问速度。
注意，⽆论数据是否存放到 Cache 中，CPU 都是先访问 Cache，只有当 Cache中找不到数据时，才会去访问内存，并把内存中的数据读⼊到 Cache 中，CPU 再从 CPU Cache 读取数据。

### Direct Mapped Cache
CPU Cache Line 会存储
- ⼀个组标记（Tag），用于标记不同的内存块
- 从内存加载过来的数据
- 一个有效位，用于标记Cache Line中的数据是否有效
例如CPU有8个Cache Line，内存分为32块，对内存地址进行取模，对于映射到同一Cache Line的内存块，通过标记识别。当有效位为0时，不会读取Cache而是从内存中读取。

CPU读取Cache时不是读取Line的整个数据，而是一个字Word。
因此需要一个偏移量来定位Line中的某个数据片段。

一个内存的访问地址为组标记+索引+偏移量组成。
- 通过内存访问地址中的索引找到对应的CacheLine
- 根据有效位判断是否从Cache中读取数据
- 通过内存访问地址中的组标记判断是否为对应的内存块
- 根据偏移量在数据块中读取对应的字
CPU Cache中的数据结构为
索引+有效位+组标记+数据块组成
![](assets/Pasted%20image%2020230718105452.png)


## Cache Miss 
如果 CPU 所要操作的数据在 CPU Cache 中的话，这样将会带来很⼤的性能提升。
**数据缓存**
例如遍历数组时，如果按照先行后列的方式遍历，在第一次读取array[0]时会将后面若干个元素一起加载到cache，这样在读取后面的元素时速度大大加快。如果按照先列后行的方式遍历，则数组元素的读取是跳跃式的，每次都无法从Cache中读取到。
**指令缓存**
如果分⽀预测可以预测到接下来要执⾏ if ⾥的指令，还是 else 指令的话，就可以「提前」把这些指令放在指令缓存中，这样 CPU 可以直接从 Cache读取到指令，于是执⾏速度就会很快。
当数组中的元素是随机的，分⽀预测就⽆法有效⼯作，⽽当数组元素都是是顺序的，则会缓存 if ⾥的指令到 Cache 中，后续 CPU 执⾏该指令从 Cache 读取。


## CPU缓存一致性
### 写直达
保持内存与 Cache ⼀致性最简单的⽅式是，把数据同时写⼊内存和 Cache 中，这种⽅法称为写直达
- 如果数据已经在 Cache ⾥⾯，先将数据更新到 Cache ⾥⾯，再写⼊到内存⾥⾯；
- 如果数据没有在 Cache ⾥⾯，就直接把数据更新到内存⾥⾯。
每次写操作都会写回到内存，性能会受到很⼤的影响。
### 写回
在写回机制中，当发⽣写操作时，新的数据仅仅被写⼊ Cache Block ⾥，只有当修改过的 Cache Block「被替换」时才需要写到内存中

- 如果当发⽣写操作时，数据已经在 CPU Cache ⾥的话，则把数据更新到 CPU Cache ⾥。同时标记CPU Cache ⾥的这个 Cache Block 为 dirty，这表示cache中的数据与内存中的不一致，但不需要写回内存。
- 如果当发⽣写操作时，数据所对应的 Cache Block ⾥存放的是「别的内存地址的数据」的话，就需要检查已存在的数据是否是脏的，即是否与内存中的不一致。如果是脏的，就把已存在的数据写回内存，然后再写入新数据。
简单来说，只有cache中的数据发生cache miss时，才需要把未命中的数据写回内存。因此只要我们的数据操作降低cache miss就使得cpu大部分时间里不需要写回内存，提高性能。


### 缓存一致性
由于L1/L2缓存时多个核心独有的，那么会带来多核⼼的缓存⼀致性。
例如A核心的cache缓存i=0，执行i++后还没有写回内存，此时B核心则从内存中读取了变量i，导致CPU核心之间的缓存不一致。
![](assets/Pasted%20image%2020230718164943.png)

- 写传播：某个 CPU 核⼼⾥的 Cache 数据更新时，必须要传播到其他核⼼的 Cache
- 事务串行化：，某个 CPU 核⼼⾥对数据的操作顺序，必须在其他核⼼看起来顺序是⼀样的
	- CPU 核⼼对于 Cache 中数据的操作，需要同步给其他 CPU 核⼼
	- 要引⼊「锁」的概念
- 
总线嗅探
当 A 号 CPU 核⼼修改了 L1 Cache 中 i 变量的值，通过总线把这个事件⼴播通知给其他所有的核⼼。
后每个 CPU 核⼼都会监听总线上的⼴播事件，并检查是否有相同的数据在⾃⼰的 L1 Cache ⾥⾯，如果有则需要更新数据到自己的 L1 Cache。
总线嗅探只保证了某个 CPU 核⼼的 Cache 更新数据这个事件能被其他 CPU 核⼼知道，不能保证事务串形化。每时每刻监听总线上的⼀切活动，不管别的核⼼的 Cache 是否缓存相同的数据都需要发出⼀个⼴播事件，会加重总线的负载。

MESI协议
Modified Exclusive Shared Invalidate
已修改 独占 共享 已失效

已失效状态，表示的是这个 Cache Block ⾥的数据已经失效了，不可以读取该状态的数据。
独占和共享状态都代表 Cache Block ⾥的数据是⼲净的。独占状态代表数据只存储在⼀个 CPU 核⼼的 Cache ⾥，可以直接⾃由地写⼊。如果有其他核⼼从内存读取了相同的数据到各⾃的 Cache ，独占的数据回返回给其他核心，并且变成共享状态。
共享状态代表着相同的数据在多个 CPU 核⼼的 Cache ⾥都有，更新 Cache ⾥⾯的数据的时候，不能直接修改，要先向所有的其他 CPU 核⼼⼴播⼀个请求，把其他核⼼的 Cache 中对应的 Cache Line 标记为⽆效状态，然后才能更新 Cache的数据，同时标记 Cache Line 为已修改
如果继续修改已修改状态的数据，则无需通知其他核心。如果已修改状态的数据要被替换，则会将数据同步到内存中。

## CPU执行任务

### 伪共享
CPU Line 是 CPU 从内存读取数据到 Cache 的单位。对数组的加载， CPU 就会加载数组⾥⾯连续的多个数据到 Cache ⾥，按照物理内存地址分布的顺序去访问元素，Cache 命中率就会很⾼。
不使⽤数组，⽽是使⽤单独的变量的时候，则会有 Cache 伪共享的问题。
当连续数组被不同的核心同时加载同一个Cache Line时，假设线程A绑定了核心A修改变量A，线程B绑定了核心B修改变量B。


![](assets/Pasted%20image%2020230718175229.png)

- 当线程A访问变量A，将Cache Line读取到Cache中，标记独占
- 当线程B访问变量B，变量AB同属于一个Cache Line，也读取到Cahce中，核心1和核心2都更新为共享。
- 核心1修改变量A，发现状态为共享，则通过总线通知核心2将该CacheLine标记为已失效，自己为已修改。
- 核心2修改变量B，发现状态为已失效，核心1有相同数据且为已修改，因此将核心1的cache line写回内存，然后核心2从内存中重新读取cache line 到cache，将修改的B更新到cache中，标记为已修改
如果 1 号和 2 号 CPU 核⼼这样持续交替的分别修改变量 A 和 B，就会重复以上步骤，使得Cache没有起到缓存的作用。
这是因为多个变量同属于一个cache line，多线程下分别修改导致数据需要不断地写回内存。

对于多个线程共享的热点数据，即经常会修改的数据，应该避免这些数据刚好在同⼀个 Cache Line 中，否则就会出现为伪共享的问题。

或者通过宏标记变量，使得相邻的两个变量不在同一个cache line中，牺牲一部分空间。


### CPU如何选择线程
在 Linux 内核中，进程和线程都是⽤ tark_struct 结构体表示的
线程的 tark_struct 结构体⾥部分资源是共享了进程已创建的资源，⽐如内存地址空间、代码段、⽂件描述符等。所以 Linux 中的线程也被称为轻量级进程。

没有创建线程的进程，是只有单个执⾏流，它被称为是主线程。

调度类
Deadline 按照 deadline 进⾏调度的，距离当前时间点最近的 deadline 的任务会被优先调度；
Reatime 对于相同优先级的任务，按先来先服务的原则，但是优先级更⾼的任务，可以抢占低优先级的任务，也就是优先级⾼的可以「插队」；
Fair 对于相同优先级的任务，轮流着运⾏，每个任务都有⼀定的时间⽚，当⽤完时间⽚的任务会被放到队列尾部，以保证相同优先级任务的公平性，但是⾼优先级的任务依然可以抢占低优先级的任务；
Fair 调度类是应⽤于普通任务，分为两种调度策略
1. 普通任务使⽤的调度策略；
2. 后台任务的调度策略，不和终端进⾏交互，因此在不影响其他需要交互的任务，可以适当降低它的优先级。
Linux 基于 CFS 的调度算法：完全公平调度
它为每个任务安排⼀个虚拟运⾏时间vruntime，如果⼀个任务在运⾏，其运⾏的越久，该任务的vruntime⾃然就会越⼤，⽽没有被运⾏的任务，vruntime 是不会变化的。在 CFS 算法调度的时候，会优先选择 vruntime 少的任务。
在计算虚拟运⾏时间 vruntime 还要考虑普通任务的权重值，注意权重值并不是优先级的值，内核中会有⼀个 nice 级别与权重值的转换表，nice 级别越低的权重值就越⼤。

虚拟运行时间vruntime+=实际运行时间delta_exec × NICE_0_LOAD / 权重

### CPU 运⾏队列

⼀个系统通常都会运⾏着很多任务，多任务的数量基本都是远超 CPU 核⼼数量，因此这时候就需要排队。
每个 CPU 都有⾃⼰的运⾏队列⽤于描述在此 CPU 上所运⾏的所有进程

其队列包含三个运⾏队列，Deadline 运⾏队列 dl_rq、实时任务运⾏队列 rt_rq 和 CFS 运⾏队列 csf_rq，
其中 csf_rq 是⽤红⿊树来描述的，按 vruntime ⼤⼩来排序的，最左侧的叶⼦节点，就是下次会被调度的任务。
![](assets/Pasted%20image%2020230718182306.png)


优先级如下：Deadline > Realtime > Fair，先从 dl_rq ⾥选择任务，然后从 rt_rq ⾥选择任务，最后从 csf_rq ⾥选择任务。

启动任务的时候，没有特意去指定优先级的话，默认情况下都是普通任务。想让某个普通任务有更多的执⾏时间，可以调整任务的 nice 值。。nice 的值能设置的范围是-20～19。
值越低，表明优先级越⾼。这是因为nice 值并不是表示优先级，⽽是表示优先级的修正数值：priority(new) = priority(old) + nice内核中，priority 的范围是0~139，值越低，优先级越⾼。nice 值调整的是普通任务的优先级。
![](assets/Pasted%20image%2020230718182528.png)



































